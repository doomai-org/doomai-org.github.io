<!DOCTYPE html>
<html>
	<head>
		<title>DoomAI</title>
		<link rel="stylesheet" href="tufte.css" />
	</head>
	<body>
		<section>
			<h1>DoomAI</h1>
			<div><a href="/">Home</a> | <a href="research.html">Research</a> | <a href="/contact.html">Contact</a></div>
			<p><a href="/research.html">Research</a>  on counter AI for <i>today</i> and what we should do if alignment fails in the <i>future</i>.</p>
			<p>Despite the name, this is not an AI Doom/Doomer organization, but actually the opposite: pro AI. </p>
			<p>We are also a pro Human organization, so we should think about worst-case scenarios in AI even if the chances are slim.</p>
			<p>We should also consider if all use cases <i>today</i> are pro Human and how to defend against bad human actors leveraging AI.</p>

			<h2>
				Issues Today
			</h2>
			<p>
				<ul>
					<li>Defending against fabricated evidence generated by AI</li>
					<li>Defending people who don't want their data trained on (ie artists)</li>
					<li>Defending against recommenders trained to maximize addiction (ie 'for you' pages)</li>
					<li>Defending against Autonomous Weapons Systems</li>
					<li>Defending against incorrectly rejecting candidates (admissions, insurance, ...)</li>
					<li>and more!</li>
				</ul>
			</p>
			<h2>
				Issues in the Future
			</h2>
			<p>
				<ul>
					<li>What do we do in the chance that alignment fails catastrophically (Doom)? We should have a backup plan even if all things go well.</li>
					<li>What if alignment works, but aligned by malicious actors (Technofedualism)?</li>
					<li>and more!</li>
				</ul>
			</p>
		</section>
	</body>
</html>
